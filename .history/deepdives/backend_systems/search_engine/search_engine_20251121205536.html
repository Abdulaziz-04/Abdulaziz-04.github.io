<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Full-Stack Search Engine: Crawl, Index, Rank, and Rerank</title>
    <link rel="stylesheet" href="../../../assets/css/styles.css" />
  </head>

  <body class="project-detail-page">
    <header class="detail-hero">
      <a class="category-back" href="../../../projects/backend_systems.html">
        &larr; Back to Projects
      </a>

      <p
        class="category-pill"
        onclick="window.location.href='../../../projects/backend_systems.html'"
      >
        Backend &amp; Distributed Logic
      </p>
      <br />

      <h1>Full-Stack Search Engine: Crawl, Index, Rank, and Rerank</h1>

      <p class="detail-summary">
        <b>
          Built an end-to-end search engine over a nuclear-safety corpus: custom
          crawler, compressed inverted indexes, classic IR ranking
          (TF-IDF/BM25/LM), graph algorithms (PageRank/HITS), vertical search UI
          with manual assessments, learning-to-rank, and ML-powered spam
          filtering.
        </b>
      </p>

      <p class="detail-summary">
        Part of CS 6200 Information Retrieval course at Northeastern: starting
        with baseline ranking, adding compression and proximity search, building
        a focused crawler, applying link analysis, running a vertical search
        assessment interface, experimenting with learning-to-rank, and closing
        with ML-based spam classification.
      </p>

      <ul class="detail-meta">
        <li>
          <strong>Tech Stack:</strong>
          Python, ElasticSearch, NumPy, scikit-learn, pandas, BeautifulSoup,
          multi-threading, custom inverted index storage
        </li>
      </ul>
    </header>

    <main class="section detail-content">
      <!-- HERO IMAGE -->
      <section class="detail-section detail-hero-gif">
        <figure class="gallery-item gallery-item--wide" style="max-width: none">
          <img
            src="./images/high_level.svg"
            alt="System architecture: crawler → indexer → ranker → reranker → UI"
            style="
              display: block;
              margin: 0 auto;
              width: 100%;
              max-width: 100%;
              height: auto;
            "
          />
          <figcaption style="text-align: center">
            High-level System Architecture
          </figcaption>
        </figure>
      </section>

      <!-- QUICK INSIGHTS -->
      <section class="detail-section">
        <h2>Quick insights</h2>
        <ul class="psr-list">
          <li>
            <strong>Goal:</strong>
            Build a demonstrable, end-to-end search stack covering crawling,
            indexing, ranking, evaluation, and ML reranking.
          </li>
          <br />
          <li>
            <strong>A1 – Baseline Ranking:</strong> Implemented TF-IDF,
            Okapi-TF, BM25, and two Language Models (Laplace, Jelinek-Mercer) on
            a stemmed corpus from scratch. <br /><br />
            <b>Key metric:</b> BM25 Mean Average Precision ≈ <b>0.31</b> based
            on trec_eval.
          </li>
          <br />
          <li>
            <strong>A2 – Index Compression &amp; Proximity:</strong> Built
            compressed inverted indexes and positional lists to enable
            phrase/proximity search. <br /><br />
            <b>Key metric:</b> Compressed index size <b>67.4&nbsp;MB</b> (down
            from 187&nbsp;MB).
          </li>
          <br />
          <li>
            <strong>A3 – Focused Crawler:</strong> Multi-threaded Priority-queue
            frontier expanding based on BFS algorithm, keyword hits, and inlink
            counts with full canonicalization and politeness. <br /><br />
            <b>Key metric:</b>
            The topic of crawling was Nuclear accidents covering different
            incidents such as Hiroshima-Nagasaki, Three-mile island, Kshytm
            disaster.
            <br />
            Collected over 160,000 documents with robots.txt compliance and a
            1-second inter-request delay.
          </li>
          <br />
          <li>
            <strong>A4 – Link Analysis:</strong> Computed PageRank and HITS over
            the custom crawled dataset to add authority/hub signals for
            reranking and getting better search results. <br />
            <br />
            <b>Key metric:</b> Convergence verified via
            <b>Shannon-entropy</b> stabilization across iterations.
          </li>
          <br />
          <li>
            <strong>A5 – Vertical Search UI &amp; Labels:</strong> Built an
            assessment interface for nuclear event queries with 3-point
            relevance judgments (0/1/2). <br />
            <br />
            <b>Key metric:</b> Collected <b>graded labels</b> enabling
            trec_eval-style scoring and LTR training.
          </li>
          <br />
          <li>
            <strong>A6 – Learning-to-Rank:</strong> Trained on features from
            five IR models plus document length; evaluated on held-out queries.
            <br />
            <br />
            <b>Key metric:</b> Best test precision <b>0.414</b>.
          </li>
          <br />
          <li>
            <strong>A7 – Spam Classification:</strong> Compared manual lexicon
            features vs full unigrams using Logistic Regression, Decision Tree,
            and Multinomial NB. <br />
            <br />
            <b>Key metric:</b> Highest <b>ROC-AUC</b> with Logistic Regression
            on <b>unigrams</b>.
          </li>
        </ul>
      </section>
      <!-- SYSTEM OVERVIEW (consolidated, non-repetitive) -->
      <section class="detail-section detail-overview">
        <h2>System overview</h2>
        <p>
          End-to-end pipeline from focused crawling to compressed indexing,
          first-pass retrieval, learning-based reranking, and a vertical search
          UI with human relevance labels. Each stage below includes its SVG
          schematic and the key responsibilities and artifacts it produces.
        </p>

        <!-- CRAWLER -->
        <h3>Crawler</h3>
        <div class="gallery-row gallery-row--one">
          <figure class="gallery-item gallery-item--wide">
            <img
              src="/mnt/data/crawler.svg"
              alt="Focused crawler architecture"
            />
            <figcaption>
              Focused PQ frontier with BFS waves, keyword hits, and inlink
              count; strict robots.txt and 1s pacing; canonicalization of URLs;
              HTML parsing to text and link graph.
            </figcaption>
          </figure>
        </div>
        <ul>
          <li>
            Frontier: priority queue blends BFS wave → keyword match score →
            inlinks for topical coverage and authority.
          </li>
          <li>
            Politeness: robots.txt honored, per-host throttling, duplicate/loop
            protection.
          </li>
          <li>
            Canonicalization: lowercase host, strip fragments/ports, normalize
            paths/params.
          </li>
          <li>
            Outputs: raw docs, outgoing links, inlink counts (for
            PageRank/HITS).
          </li>
        </ul>
        <br />

        <!-- INDEXER -->
        <h3>Indexer</h3>
        <div class="gallery-row gallery-row--one">
          <figure class="gallery-item gallery-item--wide">
            <img
              src="/mnt/data/indexer.svg"
              alt="Indexer with standard and positional inverted indexes"
            />
            <figcaption>
              Two indexes: standard inverted index for BM25/LM/TF-IDF and
              positional index for phrase/proximity; compressed with delta +
              varbyte.
            </figcaption>
          </figure>
        </div>
        <ul>
          <li>
            Preprocess: tokenize, stopword removal, stemming (stemmed &
            unstemmed variants).
          </li>
          <li>
            Standard inverted index: term → (docId, tf) for BM25, TF-IDF,
            Laplace/JM LMs.
          </li>
          <li>
            Positional index: term → (docId, [positions]) enabling phrase &
            proximity boosts.
          </li>
          <li>
            Compression: delta-encoding + variable-byte; stemmed compressed ≈
            67.4&nbsp;MB vs 187&nbsp;MB raw.
          </li>
          <li>
            Artifacts: postings, lexicon, doc lengths, corpus stats for scoring.
          </li>
        </ul>
        <br />

        <!-- RANKER -->
        <h3>Ranker</h3>
        <div class="gallery-row gallery-row--one">
          <figure class="gallery-item gallery-item--wide">
            <img
              src="/mnt/data/ranker.svg"
              alt="First-pass retrieval and link analysis signals"
            />
            <figcaption>
              First-pass retrieval with BM25/TF-IDF/Laplace/JM; optional
              proximity boosts; PageRank/HITS computed offline and used as
              features.
            </figcaption>
          </figure>
        </div>
        <ul>
          <li>
            Query pipeline: normalize → tokenize → (optional) stem → lexicon
            lookup → heap top-k.
          </li>
          <li>
            Models: BM25 (MAP ≈ 0.31), TF-IDF/Okapi-TF, Laplace LM,
            Jelinek–Mercer LM.
          </li>
          <li>
            Link analysis: PageRank & HITS on crawl graph; convergence checked
            via entropy stabilization.
          </li>
          <li>
            Outputs: top-k candidates with per-doc features (IR scores, docLen,
            PageRank/HITS, proximity).
          </li>
        </ul>
        <br />

        <!-- RERANKER -->
        <h3>Reranker</h3>
        <div class="gallery-row gallery-row--one">
          <figure class="gallery-item gallery-item--wide">
            <img
              src="/mnt/data/reranker.svg"
              alt="Learning-to-rank and spam filtering"
            />
            <figcaption>
              Learning-to-rank on IR/link/proximity features; spam down-ranking
              using Logistic Regression on unigrams.
            </figcaption>
          </figure>
        </div>
        <ul>
          <li>
            Features: BM25, TF-IDF, Laplace, JM, docLen, PageRank, HITS,
            proximity.
          </li>
          <li>
            Labels: 3-point relevance (0/1/2) from UI assessments; held-out
            queries for testing.
          </li>
          <li>
            Result: best test precision ≈ 0.414 with LTR; spam ROC-AUC led by
            Logistic (unigrams).
          </li>
          <li>Output: final reranked list with spam mitigation.</li>
        </ul>
        <br />

        <!-- SEARCH UI -->
        <h3>Search engine UI</h3>
        <div class="gallery-row gallery-row--one">
          <figure class="gallery-item gallery-item--wide">
            <img
              src="/images/ui.svg"
              alt="Vertical search UI with assessment loop"
            />
            <figcaption>
              Vertical search interface for nuclear-safety corpus with snippets,
              scores, and 0/1/2 relevance labeling; data feeds trec_eval and LTR
              training.
            </figcaption>
          </figure>
        </div>
        <ul>
          <li>
            Experience: query, results with snippets/scores, per-result label
            control.
          </li>
          <li>
            Evaluation: trec_eval metrics (MAP, P@K, NDCG) over labeled
            judgments.
          </li>
          <li>
            Telemetry: query stats and feature dumps for iterative training and
            analysis.
          </li>
        </ul>
      </section>

      <!-- RESPONSE EXAMPLES GALLERY -->
      <section class="detail-section detail-gallery">
        <h2>Sample outputs</h2>
        <div class="gallery-row gallery-row--two">
          <figure class="gallery-item">
            <a href="./images/search-results-placeholder.png" target="_blank">
              <img
                src="./images/search-results-placeholder.png"
                alt="Search results list with highlights and BM25 scores"
              />
            </a>
            <figcaption>
              BM25-ranked results with snippet highlights and scores.
            </figcaption>
          </figure>

          <figure class="gallery-item">
            <a href="./images/index-stats-placeholder.png" target="_blank">
              <img
                src="./images/index-stats-placeholder.png"
                alt="Index size comparison: compressed vs decompressed"
              />
            </a>
            <figcaption>
              Index size and proximity index metrics (67.4 MB compressed vs. 187
              MB decompressed stemmed).
            </figcaption>
          </figure>
        </div>

        <div class="gallery-row gallery-row--two">
          <figure class="gallery-item">
            <a href="./images/ltr-placeholder.png" target="_blank">
              <img
                src="./images/ltr-placeholder.png"
                alt="Learning-to-rank precision chart"
              />
            </a>
            <figcaption>
              Learning-to-rank precision curve; best test precision 0.414.
            </figcaption>
          </figure>

          <figure class="gallery-item">
            <a href="./images/spam-roc-placeholder.png" target="_blank">
              <img
                src="./images/spam-roc-placeholder.png"
                alt="ROC curve for spam classifier"
              />
            </a>
            <figcaption>
              Spam classifier ROC-AUC: Logistic with full unigrams leads.
            </figcaption>
          </figure>
        </div>
      </section>

      <!-- ARCHITECTURE & DESIGN -->
      <section class="detail-section">
        <h2>Architecture and design choices</h2>
        <p>
          Designed to showcase both IR fundamentals and practical trade-offs:
          efficiency (compression, proximity), relevance (BM25 vs LM), authority
          (PageRank/HITS), and learning-based reranking.
        </p>

        <ul>
          <li>
            <b>Index variants:</b>
            Stemmed/unstemmed, compressed/decompressed; proximity lists for
            phrase queries (proximity Average Precision: 0.2313 for stemmed and
            0.1814 for unstemmed).
          </li>
          <br />
          <li>
            <b>Candidate generation:</b>
            Heap-based top-k retrieval; doc-length normalized and cached;
            multilingual stopword handling via ElasticSearch analyzers.
          </li>
          <br />
          <li>
            <b>Link signals:</b>
            PageRank on merged crawled index; HITS hubs/authorities skewed
            toward Wikipedia for topical queries, improving diversity over
            PageRank.
          </li>
          <br />
          <li>
            <b>Assessment loop:</b>
            Vertical search UI captures 3-point relevance labels (0/1/2) per
            result; stored as space-separated text for trec_eval-style scoring.
          </li>
          <br />
          <li>
            <b>LTR features:</b>
            Five IR scores (TF-IDF, Okapi-TF, BM25, Laplace, JM) + doc length;
            per-query split via random shuffle; balanced train/test precision.
          </li>
          <br />
          <li>
            <b>Spam pipeline:</b>
            Manual spam lexicon vs. full unigrams; CountVectorizer sparse
            matrices; compared Logistic, Decision Tree, Multinomial NB; unigrams
            outperformed manual lists, highlighting feature coverage.
          </li>
        </ul>
      </section>

      <!-- EDGE CASES & ROBUSTNESS -->
      <section class="detail-section">
        <h2>Edge cases and robustness</h2>
        <p>
          Focused on correctness and resilience across crawling, parsing, and
          scoring.
        </p>

        <ul>
          <li>
            <b>Canonicalization &amp; politeness:</b>
            Lowercasing, port stripping, fragment removal, duplicate slash
            cleanup; robots.txt honored; 1s inter-request delay; single-request
            per URL guard.
          </li>
          <br />
          <li>
            <b>Frontier fairness:</b>
            Priority queue blends BFS wave number, keyword hits, and inlink
            counts to avoid topic drift and reward authoritative seeds.
          </li>
          <br />
          <li>
            <b>Query parsing:</b>
            Token normalization, length normalization; language models tuned for
            sensitivity; BM25 for stability on sparse queries.
          </li>
          <br />
          <li>
            <b>Assessment UX:</b>
            Relevance dropdown prevents empty labels; results persisted on
            submit; clickable URLs to verify content before rating.
          </li>
          <br />
          <li>
            <b>Model sanity checks:</b>
            Compared manual spam features vs. full vocab to avoid underfitting;
            monitored over/underfit via precision bounds.
          </li>
        </ul>
      </section>

      <!-- SCALING & FUTURE ENHANCEMENTS -->
      <section class="detail-section">
        <h2>Scaling and future enhancements</h2>
        <p>Ready to grow from coursework to production-grade demo.</p>

        <ul>
          <li>
            <b>Sharding &amp; cache:</b>
            Shard the index by domain/time; add postings and query result cache
            (Redis) to reduce P95 latency.
          </li>
          <br />
          <li>
            <b>Reranking depth:</b>
            Introduce XGBoost with expanded features (PageRank, HITS, click
            priors) and cross-validation over query folds.
          </li>
          <br />
          <li>
            <b>Quality signals:</b>
            Add autocomplete with prefix tries, spell correction via k-grams and
            snippet generator with positional windows.
          </li>
          <br />
          <li>
            <b>Deployment:</b>
            Containerize crawler, indexer, API, and UI; use docker-compose for a
            single-node demo; add health checks and telemetry.
          </li>
          <br />
          <li>
            <b>Spam hardening:</b>
            Add character n-grams for obfuscation, URL reputation features, and
            threshold tuning for precision/recall trade-offs.
          </li>
        </ul>
      </section>

      <!-- IMPACT & LEARNINGS -->
      <section class="detail-section detail-impact">
        <h2>Impact and learnings</h2>
        <ul>
          <li>
            <b>IR depth:</b>
            Got the opportunity to design ElasticSearch from scratch and
            hands-on with classic retrieval models, proximity, compression, and
            graph algorithms, understanding when BM25 beats LMs and how link
            signals diversify results.
          </li>
          <br />
          <li>
            <b>Evaluation mindset:</b>
            Built a full relevance loop: manual labels, trec_eval metrics,
            precision tracking, and feature-driven LTR gains.
          </li>
          <br />
          <li>
            <b>ML integration:</b>
            Combined sparse text features with traditional IR scores; compared
            manual heuristics vs. data-driven vocabularies for spam detection.
          </li>
          <br />
          <li>
            <b>Systems thinking:</b>
            From crawling politeness to storage trade-offs, every stage is
            modular and independently testable for clearer scaling paths.
          </li>
        </ul>
      </section>
    </main>
  </body>
</html>
